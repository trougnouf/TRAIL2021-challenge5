"""Train loop for RAF model with PyTorch Lightning."""

import os
import random
import sys
import datetime
import yaml
import configargparse
import torch
from torch import nn
import torchvision
import numpy as np
from pl_bolts.models.self_supervised import SwAV

import env

sys.path.append("..")
from emotions.tools import prep_raf_dataset, prep_kdef_dataset

random.seed(42)
MODELS_DPATH = os.path.join("..", "models", "emotions")

DS_BUILDERS = {
    "raf": prep_raf_dataset.make_raf_ImageFolder_struct,
    "kdef": prep_kdef_dataset.make_KDEF_ImageFolder_struct,
}


def parse_arguments():
    """Parse config, return args."""
    parser = configargparse.ArgumentParser(
        description=__doc__,
        default_config_files=[os.path.join("configs", "defaults.yaml")],
        config_file_parser_class=configargparse.YAMLConfigFileParser,
    )
    parser.add_argument(
        "--config",
        is_config_file=True,
        dest="config",
        required=False,
        help="config in yaml format",
    )
    parser.add_argument(
        "--expname", help="experiment name (default is autogenerated from datetime)"
    )
    parser.add_argument("--n_epochs", type=int, help="Number of training epochs")
    parser.add_argument("--patience", type=int, help="How many epochs before LR decays")
    parser.add_argument(
        "--lr_decay",
        type=float,
        help="LR decay (multiply LR by this every [patience] epochs.)",
    )
    parser.add_argument(
        "--train_ds_names",
        nargs="+",
        help="Dataset names (directories in ../datasets/emotions/train/)",
    )
    parser.add_argument(
        "--test_ds_names",
        nargs="+",
        help="Dataset names (directories in ../datasets/emotions/test)",
    )
    parser.add_argument(
        "--device",
        help="Device number used if cuda is detected",
    )
    return parser.parse_args()


def get_dataloaders(
    train_ds_names,
    test_ds_names,
    image_size=(224, 224),
    batch_size=32,
    ds_root=env.DS_ROOT,
):
    """
    Load a RAF dataset from data_dpath containing train_* and test_* images.

    Return train, test, val (10% of train) dataloaders.
    """
    train_sets = list()
    test_sets = list()
    for ds_name in train_ds_names:
        if not os.path.isdir(os.path.join(env.DS_ROOT, "train", ds_name)):
            print(f"Dataset not found; building with {DS_BUILDERS[ds_name]}")
            DS_BUILDERS[ds_name]()

        train_set = torchvision.datasets.ImageFolder(
            root=os.path.join(ds_root, "train", ds_name),
            transform=torchvision.transforms.Compose(
                [
                    torchvision.transforms.Resize(image_size),
                    torchvision.transforms.ToTensor(),
                ]
            ),
        )
        train_sets.append(train_set)
    for ds_name in test_ds_names:
        test_set = torchvision.datasets.ImageFolder(
            root=os.path.join(ds_root, "test", ds_name),
            transform=torchvision.transforms.Compose(
                [
                    torchvision.transforms.Resize(image_size),
                    torchvision.transforms.ToTensor(),
                ]
            ),
        )
        test_sets.append(test_set)
    train_combisets = torch.utils.data.ConcatDataset(train_sets)
    test_combisets = torch.utils.data.ConcatDataset(test_sets)

    train_loader = torch.utils.data.DataLoader(
        train_combisets, batch_size=batch_size, shuffle=True
    )
    test_loader = torch.utils.data.DataLoader(
        test_combisets, batch_size=batch_size, shuffle=True
    )
    return train_loader, test_loader


def train(
    dataset,
    model,
    loss_function,
    optimizer,
    device,
    n_epochs,
    val_dataset,
    save_dpath,
    patience,
    lr_decay,
):
    """Training loop."""
    print("Started training")
    writer = torch.utils.tensorboard.SummaryWriter(
        os.path.join(save_dpath, "tensorboard")
    )
    for epoch in range(n_epochs):  # loop over the dataset multiple times
        # simple lr decay every patience epochs
        if epoch % patience == 0 and epoch > 0:
            for param_group in optimizer.param_groups:
                param_group["lr"] *= lr_decay
                print(f'train: reduced lr to {param_group["lr"]}')
        writer.add_scalar("lr", optimizer.param_groups[0]["lr"], epoch)
        running_loss = 0.0
        evaluation = []
        for i, data in enumerate(dataset):
            inputs, labels = data
            inputs = inputs.to(device)
            labels = labels.to(device)

            # forward + backward + optimize
            optimizer.zero_grad()
            outputs = model(inputs)[1]
            loss = loss_function(outputs, labels)
            loss.backward()
            optimizer.step()

            evaluation.append((outputs.argmax(1) == labels).cpu().sum().item())

            # print statistics
            running_loss += loss.item()
            if i % 10 == 9:
                running_loss = running_loss / 10
                print(
                    "[%d, %5d/%d] loss: %.3f"
                    % (epoch + 1, i + 1, len(dataset), running_loss)
                )
                writer.add_scalar(
                    "Running Loss/train", running_loss, epoch * len(dataset) + i
                )
                writer.add_scalar(
                    "Accuracy/train", np.mean(evaluation) / 32, epoch * len(dataset) + i
                )
                evaluation = []
                running_loss = 0.0

        validation_loss = 0.0
        evaluation = []
        running_loss = 0.0

        num_correct = 0
        num_items = 0
        for _, data in enumerate(val_dataset):
            inputs, labels = data
            inputs = inputs.to(device)
            labels = labels.to(device)

            # forward
            outputs = model(inputs)[1]
            loss = loss_function(outputs, labels)
            validation_loss += loss.item()
            num_correct += (outputs.argmax(1) == labels).sum().cpu().item()
            num_items += len(outputs)

        validation_loss = validation_loss / len(val_dataset)
        accuracy = num_correct / num_items
        torch.save(
            model.state_dict(), os.path.join(save_dpath, f"resnet50_swav_{epoch}.pth")
        )
        print(
            "[Epoch %d / %d],  Validation loss: %.3f"
            % (epoch + 1, n_epochs, validation_loss)
        )
        writer.add_scalar("Running Loss/val", validation_loss, epoch)
        writer.add_scalar("Accuracy/val", accuracy, epoch)
    writer.flush()
    writer.close()
    print("Finished Training")


if __name__ == "__main__":
    args = parse_arguments()
    if not args.expname:
        args.expname = datetime.datetime.now().replace(microsecond=0).isoformat()
    save_dpath = os.path.join(MODELS_DPATH, args.expname)
    os.makedirs(save_dpath, exist_ok=True)
    with open(os.path.join(save_dpath, "config.yaml"), "w") as fp:
        yaml.dump(vars(args), fp)

    NUM_CLASSES = 7
    train_set, test_set = get_dataloaders(args.train_ds_names, args.test_ds_names)
    device = torch.device(
        f"cuda:{args.device}" if torch.cuda.is_available() else "cpu"
    )  # TODO parametrable GPU number
    WEIGHT_PATH = "https://pl-bolts-weights.s3.us-east-2.amazonaws.com/swav/swav_imagenet/swav_imagenet.pth.tar"
    model = SwAV.load_from_checkpoint(WEIGHT_PATH, strict=True).model
    model.prototypes = nn.Linear(128, NUM_CLASSES)
    model = model.to(device)

    loss_function = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters())
    train(
        dataset=train_set,
        model=model,
        loss_function=loss_function,
        optimizer=optimizer,
        device=device,
        n_epochs=args.n_epochs,
        val_dataset=test_set,
        save_dpath=save_dpath,
        patience=args.patience,
        lr_decay=args.lr_decay,
    )
