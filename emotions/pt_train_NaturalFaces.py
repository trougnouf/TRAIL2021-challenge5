import os
import random
import sys
import datetime
import yaml
import configargparse
import torch
from torch import nn
import torchvision
import numpy as np
from pl_bolts.models.self_supervised import SwAV, SimCLR, SimSiam

import env

sys.path.append("..")
sys.path.append("lightning_logs")
from emotions.tools import prep_raf_dataset, prep_kdef_dataset

random.seed(42)
MODELS_DPATH = os.path.join("lightning_logs", "models", "emotions")

DS_BUILDERS = {
    "raf": prep_raf_dataset.make_raf_ImageFolder_struct,
    "kdef": prep_kdef_dataset.make_KDEF_ImageFolder_struct,
}


def parse_arguments():
    """Parse config, return args."""
    parser = configargparse.ArgumentParser(
        description=__doc__,
        default_config_files=[os.path.join("configs", "defaults.yaml")],
        config_file_parser_class=configargparse.YAMLConfigFileParser,
    )
    parser.add_argument(
        "--config",
        is_config_file=True,
        dest="config",
        required=False,
        help="config in yaml format",
    )
    parser.add_argument(
        "--expname", help="experiment name (default is autogenerated from datetime)"
    )
    parser.add_argument("--n_epochs", type=int, help="Number of training epochs")
    parser.add_argument("--patience", type=int, help="How many epochs before LR decays")
    parser.add_argument(
        "--lr_decay",
        type=float,
        help="LR decay (multiply LR by this every [patience] epochs.)",
    )
    parser.add_argument(
        "--train_ds_names",
        nargs="+",
        help="Dataset names (directories in ../datasets/emotions/train/)",
    )
    parser.add_argument(
        "--test_ds_names",
        nargs="+",
        help="Dataset names (directories in ../datasets/emotions/test)",
    )
    parser.add_argument(
        "--device",
        help="Device number used if cuda is detected",
    )
    parser.add_argument(
        "--no_pretrain",
        action="store_true",
        help="Train from scratch",
    )
    parser.add_argument(
        '--model',
        help='Which model to train, try Swav, SimCLR or SimSiam',
    )
    parser.add_argument(
        '--pretrained_dataset',
        help='Which dataset the SSL model has been trained on, try ImageNet or FFHQ',
    )
    parser.add_argument(
        '--batch_size',
        type = int,
    )
    return parser.parse_args()


def get_dataloaders(
    train_ds_names,
    test_ds_names,
    image_size=(224, 224),
    batch_size=16,
    ds_root=env.DS_ROOT,
):
    """
    Get train and test data loaders.

    Based on directories in ../datasets/[train or test].
    """
    train_sets = []
    test_sets = []
    for ds_name in train_ds_names:
        if not os.path.isdir(os.path.join(env.DS_ROOT, "train", ds_name)):
            print(f"Dataset not found; building with {DS_BUILDERS[ds_name]}")
            DS_BUILDERS[ds_name]()

        train_set = torchvision.datasets.ImageFolder(
            root=os.path.join(ds_root, "train", ds_name),
            transform=torchvision.transforms.Compose(
                [
                    torchvision.transforms.Resize(image_size),
                    torchvision.transforms.ToTensor(),
                ]
            ),
        )
        train_sets.append(train_set)
    for ds_name in test_ds_names:
        test_set = torchvision.datasets.ImageFolder(
            root=os.path.join(ds_root, "test", ds_name),
            transform=torchvision.transforms.Compose(
                [
                    torchvision.transforms.Resize(image_size),
                    torchvision.transforms.ToTensor(),
                ]
            ),
        )
        test_sets.append(test_set)
    train_combisets = torch.utils.data.ConcatDataset(train_sets)
    test_combisets = torch.utils.data.ConcatDataset(test_sets)

    train_loader = torch.utils.data.DataLoader(
        train_combisets, batch_size=batch_size, shuffle=True
    )
    test_loader = torch.utils.data.DataLoader(
        test_combisets, batch_size=batch_size, shuffle=True
    )
    return train_loader, test_loader


def weights_init(m):
    if isinstance(m, nn.Conv2d):
        torch.nn.init.xavier_uniform(m.weight.data)
    if isinstance(m, nn.Linear):
        torch.nn.init.xavier_uniform(m.weight)
        m.bias.data.fill_(0.01)


def train(
    dataset,
    model,
    loss_function,
    optimizer,
    device,
    n_epochs,
    val_dataset,
    save_dpath,
    patience,
    lr_decay,
):
    """Training loop."""
    print("Started training")
    writer = torch.utils.tensorboard.SummaryWriter(
        os.path.join(save_dpath, "tensorboard")
    )
    for epoch in range(n_epochs):  # loop over the dataset multiple times
        # simple lr decay every patience epochs
        if epoch % patience == 0 and epoch > 0:
            for param_group in optimizer.param_groups:
                param_group["lr"] *= lr_decay
                print(f'train: reduced lr to {param_group["lr"]}')
        writer.add_scalar("lr", optimizer.param_groups[0]["lr"], epoch)
        running_loss = 0.0
        evaluation = []
        for i, data in enumerate(dataset):
            inputs, labels = data
            inputs = inputs.to(device)
            labels = labels.to(device)

            # forward + backward + optimize
            optimizer.zero_grad()
            outputs = model(inputs)[0]
            loss = loss_function(outputs, labels)
            loss.backward()
            optimizer.step()

            evaluation.append((outputs.argmax(1) == labels).cpu().sum().item())

            # print statistics
            running_loss += loss.item()
            if i % 10 == 9:
                running_loss = running_loss / 10
                print(
                    "[%d, %5d/%d] loss: %.3f"
                    % (epoch + 1, i + 1, len(dataset), running_loss)
                )
                writer.add_scalar(
                    "Running Loss/train", running_loss, epoch * len(dataset) + i
                )
                writer.add_scalar(
                    "Accuracy/train", np.mean(evaluation) / 32, epoch * len(dataset) + i
                )
                evaluation = []
                running_loss = 0.0

        validation_loss = 0.0
        evaluation = []
        running_loss = 0.0

        num_correct = 0
        num_items = 0
        for _, data in enumerate(val_dataset):
            inputs, labels = data
            inputs = inputs.to(device)
            labels = labels.to(device)

            # forward
            outputs = model(inputs)[0]
            loss = loss_function(outputs, labels)
            validation_loss += loss.item()
            num_correct += (outputs.argmax(1) == labels).sum().cpu().item()
            num_items += len(outputs)

        validation_loss = validation_loss / len(val_dataset)
        accuracy = num_correct / num_items

        torch.save(
            model.state_dict(), os.path.join(save_dpath, f"resnet50_swav_{epoch}.pth")
        )
        print(
            "[Epoch %d / %d],  Validation loss: %.3f"
            % (epoch + 1, n_epochs, validation_loss)
        )
        writer.add_scalar("Running Loss/val", validation_loss, epoch)
        writer.add_scalar("Accuracy/val", accuracy, epoch)
    writer.flush()
    writer.close()
    print("Finished Training")


if __name__ == "__main__":
    args = parse_arguments()
    if not args.expname:
        args.expname = datetime.datetime.now().replace(microsecond=0).isoformat()
    save_dpath = os.path.join(MODELS_DPATH, args.expname)
    os.makedirs(save_dpath, exist_ok=True)
    with open(os.path.join(save_dpath, "config.yaml"), "w") as fp:
        yaml.dump(vars(args), fp)

    NUM_CLASSES = 7
    train_set, test_set = get_dataloaders(args.train_ds_names, args.test_ds_names, batch_size=args.batch_size)
    device = torch.device(f"cuda:{args.device}" if torch.cuda.is_available() else "cpu")
    assert(args.model in ('Swav', 'SimCLR', 'SimSiam')), 'Invalid model name, try Swav or SimCLR'
    assert(args.pretrained_dataset in ('ImageNet','FFHQ')), 'Invalied pre_training dataset, try ImageNet or FFHQ'
    if args.model == 'Swav' and args.pretrained_dataset == 'ImageNet':
        WEIGHT_PATH = "https://pl-bolts-weights.s3.us-east-2.amazonaws.com/swav/swav_imagenet/swav_imagenet.pth.tar"
    elif args.model == 'SimCLR' and args.pretrained_dataset == 'ImageNet':
        WEIGHT_PATH = 'https://pl-bolts-weights.s3.us-east-2.amazonaws.com/simclr/bolts_simclr_imagenet/simclr_imagenet.ckpt'
    elif args.model == 'SimCLR' and args.pretrained_dataset == 'FFHQ':
        WEIGHT_PATH = '/home/imagedpt/doodling/TRAIL2021-challenge5/emotions/SimCLR_NaturalFaces_e100.ckpt'
    elif args.model == 'SimSiam' and args.pretrained_dataset == 'FFHQ':
        WEIGHT_PATH = '/home/imagedpt/doodling/TRAIL2021-challenge5/emotions/SiamSiam_NaturalFaces_e80.ckpt'
    if args.model == 'Swav':
        model = SwAV.load_from_checkpoint(WEIGHT_PATH, strict=True).model
        model.prototypes = nn.Linear(128, NUM_CLASSES)
    elif args.model == 'SimCLR':
        model = SimCLR.load_from_checkpoint(WEIGHT_PATH, strict=False).encoder
        model.fc = torch.nn.Linear(in_features=2048, out_features=NUM_CLASSES, bias=True)
    else:
        model = SimSiam.load_from_checkpoint(WEIGHT_PATH, strict=True).online_network
        model = model.encoder
        model.fc = torch.nn.Linear(in_features=2048, out_features=NUM_CLASSES, bias=True)
        #To be implemented
    model = model.to(device)

    if args.no_pretrain:
        print("Reseting weights.")
        model.apply(weights_init)

    loss_function = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters())
    train(
        dataset=train_set,
        model=model,
        loss_function=loss_function,
        optimizer=optimizer,
        device=device,
        n_epochs=args.n_epochs,
        val_dataset=test_set,
        save_dpath=save_dpath,
        patience=args.patience,
        lr_decay=args.lr_decay,
    )
